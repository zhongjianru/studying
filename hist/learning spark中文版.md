#### 第三章 RDD ?程Spark ?理?据的核心抽象：* ?性分布式?据集（RDD）* 所有?象的一?不可?的分布式集合* RDD 中的?据自?分布到整?集群，并??行的操作并行化* 可以包含任何 Python，Java，Scala ?象?型，包括用?自定??型步?：* ?建 RDD：加?外部?据集或在??程序中分?一??象集合（如 list 或 set）* ?已存在的 RDD 做??（transformation）：map() 和 filter()，返回一?新的 RDD* ? RDD ?用某些操作??算得到一??果（action）：count() 和 first()，返回其他?型?果（直到此?才真正?行??）```// 步?示例（python）lines = sc.textFile("README.md")  // 新建pythonlines = lines.filter(lamda line: "Python" in line)  // ??pythonlines.first()  // 并行?算（lazy?算，不需要?描整?文件）pythonLines.persist()  // 持久化 RDD 到?存中，以便以后重用pythonLines.count()  // 在第一??作之前?用持久化pythonLines.unpersist()  // 手??放?存// 1、python// ?建 RDDlines = sc.parallelize(["pandas", "i like pandas"])  // 新建lines = sc.textFile("/path/to/README.md")  // ?取// ??日志inputRDD = sc.textFile("log.txt")errorsRDD = inputRDD.filter(lambda x: "error" in x)warningsRDD = inputRDD.filter(lamda x: "warning" in x)badLinesRDD = errorsRDD.union(warningsRDD)  // 合并??信息// ????// take ?取少量元素，collect ?取全部元素（不适合?大?据集使用）print "Input had" + badLinesRDD.count() + "concerning lines"print "Here are 10 examples:"for line in badLinesRDD.take(10):    print line// ????word = rdd.filter(lambda s: "error" in s)def containsError(s):    return "errors" in sword = rdd.filter(containsError)// ?算平方值nums = sc.parallelize([1, 2, 3, 4])squared = nums.map(lambda x: x * x).collect()for num in squared:    print "%i " % (num)// ?文本分割???lines = sc.parallelize(["hello world", "hi"])words = lines.flatMap(lambda line: line.split(" "))words.first()  # returns "hello"// ?集合操作rdd.distinct()  # 去重：操作很昂?，需要?所有的?据通?网??行 Shuffling 以确保唯一性rdd.union(other)  # 并集（可能有重复元素）rdd.intersection(other)  # 交集（同?去除所有重复元素），需要 shufflerdd.subtract(other)  # ?值，需要 shufflerdd.cartesian(other)  # 笛卡??rdd = sc.parallelize({1, 2, 3, 3})rdd.map(x => x + 1)  # ?用函? {2, 3, 4, 4}rdd.flatMap( x => x.to(3) )  # ?用函? {1, 2, 3, 2, 3, 3, 3}rdd.filter( x => x != 1 )  # ?件?? {2, 3, 3}rdd.distinct()  # 去重 {1, 2, 3}rdd.sample(false, 0.5)  # 抽? Nondeterministicrdd.collect()  # 返回所有元素 {1, 2, 3, 3}rdd.count()  # ?? 4rdd.countByValue()  # 元素?? {(1, 1), (2, 1), (3, 2)}rdd.take(2)  # 返回元素 {1, 2}rdd.top(2)  # 返回前面元素 {3, 3}rdd.takeOrder(2)(myOrdering)  # 返回?定?序元素 {3, 3}rdd.takeSample(false, 1)  # ?机返回元素 Nondeterministicrdd.reduce((x,y) => x+y)  # 并行合并元素 9rdd.fold(0)((x,y) => x+y)  # ?定初值 9rdd.aggregate((0,0) ((x,y)=>(x._1+y,x._2+1),(x,y)=>(x._1+y._1,x._2+y._2)))  # 返回不同?型 (9,4)rdd.foreach(func)  # ?每?元素?用函?rdd = sc.parallelize({1, 2, 3})other = sc.parallelize({3, 4, 5})rdd.union(other)  # {1, 2, 3, 3, 4, 5}rdd.intersection(other)  # {3}rdd.subtract(other)  # {1, 2}rdd.cartesian(other)  # {(1, 3), (1, 4), ..., (3,5)}// 求和sum = rdd.reduce(lambda x, y: x + y)// 求均值// aggregate：可以返回不同?型 RDD，相?于先 map ?元素和 1 再 foldersumCount = nums.aggregate((0, 0),            (lambda acc, value: (acc[0] + value, acc[1] + 1),            (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))))return sumCount[0] / float(sumCount[1])// 2、scala// ?建 RDDval lines = sc.parallelize(List("pandas", "i like pandas"))val lines = sc.textFile("/path/to/README.md")// ??日志val inputRDD = sc.textFile("log.txt")val errorsRDD = inputRDD.filter(line => line.contains("error"))// ????println("Input had" + badLinesRDD.count() + "concerning lines")println("Here are 10 examples:")badLinesRDD.take(10).foreach(println)// ??函?class SearchFunctions(val query: String) {    def isMatch(s: String): Boolean = {        s.contains(query)    }    def getMatchesFunctionReference(rdd: RDD[String]): RDD[String] = {        // Problem: "isMatch" means "this.isMatch", so we pass all of "this"        rdd.map(isMatch)    }    def getMatchesFieldReference(rdd: RDD[String]): RDD[String] = {        // Problem: "query" means "this.query", so we pass all of "this"        rdd.map(x => x.split(query))    }    def getMatchesNoReference(rdd: RDD[String]): RDD[String] = {        // Safe: extract just the field we need into a local variable        val query_ = this.query        rdd.map(x => x.split(query_))    }}// ?算平方值val input = sc.parallelize(List(1, 2, 3, 4))val result = input.map(x => x * x)println(result.collect().mkString(","))// ?文本分割???val lines = sc.parallelize(List("hello world", "hi"))val words = lines.flatMap(line => line.split(" "))words.first()  // returns "hello"// 求和val sum = rdd.reduce((x, y) => x + y)// 求均值val result = input.aggregate((0, 0))(            (acc, value) => (acc._1 + value, acc._2 + 1),            (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2))val avg = result._1 / result._2.toDouble// 3、Java// ?建 RDDJavaRDD<String> lines = sc.parallelize(Arrays.asList("pandas", "i like pandas"));JavaRDD<String> lines = sc.textFile("/path/to/README.md");// ??日志JavaRDD<String> lines = sc.textFile("log.txt");JavaRDD<String> errorsRDD = inputRDD.filter({New Function<String, Boolean>(){Public Boolean call(string x) { return x.contains("error"); }}});// ????System.out.println("Input had" + badLinesRDD.count() + "concerning lines");System.out.println("Here are 10 examples:");for (String line: badLinesRDD.take(10)){    System.out.println(line);}// ??函?（?部匿名?）RDD<String> errors = lines.filter(new Function<String, Boolean>() {    public Boolean call(String x) { return x.contains("error"); }});// ??函?（命名?）class ContainsError implements Function<String, Boolean>() {    public Boolean call(String x) { return x.contains("error"); }}RDD<String> errors = lines.filter(new ContainsError());// ???的函??class Contains implements Function<String, Boolean>() {    private String query;    public Contains(String query) { this.query = query; }    public Boolean call(String x) { return x.contains(query); }}RDD<String> errors = lines.filter(new Contains("error"));// ??函?（lambda 表?式）RDD<String> errors = lines.filter(s -> s.contains("error"));// ?算平方值JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 4));JavaRDD<Integer> result = rdd.map(new Function<Integer, Integer>() {    public Integer call(Integer x) { return x*x; }});System.out.println(StringUtils.join(result.collect(), ","));// ?文本分割???JavaRDD<String> lines = sc.parallelize(Arrays.asList("hello world", "hi"));JavaRDD<String> words = lines.flatMap(new FlatMapFunction<String, String>() {    public Iterable<String> call(String line) {        return Arrays.asList(line.split(" "));    }});words.first();  // returns "hello"// 求和Integer sum = rdd.reduce(new Function2<Integer, Integer, Integer>() {    public Integer call(Integer x, Integer y) { return x + y; }});// 求均值class AvgCount implements Serializable {    public AvgCount(int total, int num) {        this.total = total;        this.num = num;    }    public int total;    public int num;    public double avg() {        return total / (double) num;    }}Function2<AvgCount, Integer, AvgCount> addAndCount =    new Function2<AvgCount, Integer, AvgCount>() {        public AvgCount call(AvgCount a, Integer x) {            a.total += x;            a.num += 1;            return a;        }};Function2<AvgCount, AvgCount, AvgCount> combine =    new Function2<AvgCount, AvgCount, AvgCount>() {        public AvgCount call(AvgCount a, AvgCount b) {            a.total += b.total;            a.num += b.num;            return a;        }};AvgCount initial = new AvgCount(0, 0);AvgCount result = rdd.aggregate(initial, addAndCount, combine);System.out.println(result.avg());```#### 第四章 ?理?值?```// 1、python// ?建 pair RDDpairs = lines.map(lambda x: (x.split(" ")[0], x))rdd = sc.parallelize(pairs)// ??rdd = sc.parallelize({(1,2), (3,4), (3,6)})other = sc.parallelize({(3,9)})rdd.reduceByKey((x,y)=>x+y)  # 按相同的?合并 {(1,2),(3,10)}rdd.groupByKey()  # 按相同的?分? {(1,[2]),(3,[4,6])}rdd.mapValues(x=>x+1)  # ?不?，值?用函? {(1,3),(3,5),(3,7)}rdd.flatMapValues(x=>(x to 5))  # {(1,2),(1,3),(1,4),(1,5),(3,4),(3,5)}rdd.keys()  # 返回所有? {1,3,3}rdd.values()  # 返回所有值 {2,4,6}rdd.sortByKey()  # 按??行排序 {(1,2),(3,4),(3,6)}rdd.subtractByKey(other)  # ?值 {(1, 2)}rdd.join(other)  # ??接 {(3,(4,9)),(3,(6,9))}rdd.rightOutJoin(other)  # 右?接 {(3,(Some(4),9)),(3,(Some(6),9))}rdd.leftOutJoin(other)  # 左?接 {(1,(2,None)),(3,(4,Some(9))),(3,(6,Some(9)))}rdd.cogroup(other)  # ?共同的?分? {(1,([2],[])),(3,([4,6],[9]))}// ??值result = pairs.filter(lambda keyValue: len(keyValue[1]) < 20)// ??值（?者等价）mapValues(func)map{case (x, y) => (x, func(y))}// 聚合：在机器本地自?合并，再?每???行全局?算，不指定合并函?// 是??而不是操作，因?返回的是 RDDreduceByKey()  // 与 reduce() 不同foldByKey()  // 与 fold() 不同combineByKey()  // 允?自定?合并行?// ?算?的均值rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))// ?算?的均值（方法2）sumCount = nums.combineByKey((lambda x: (x,1)),            (lambda x, y: (x[0] + y, x[1] + 1)),            (lambda x, y: (x[0] + y[0], x[1] + y[1])))sumCount.map(lambda key, xy: (key, xy[0]/xy[1])).collectAsMap()// Word countrdd = sc.textFile("s3://...")words = rdd.flatMap(lambda x: x.split(" "))result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)// 自定?并行度data = [("a", 3), ("b", 4), ("a", 1)]sc.parallelize(data).reduceByKey(lambda x, y: x + y) # Default parallelismsc.parallelize(data).reduceByKey(lambda x, y: x + y, 10) # Custom parallelism// 改?分?repartition()  # 跨网???据?行shuffle，并?建一?新的分?coalesce()  # 可以避免?据移?，但是?限于?少 RDD 的分??的情?rdd.getPartitions()  # 查看分??// ?据分?rdd.groupByKey()  # ?据中已?有主?，得到 RDD ?型?[K, Iterable[V]]// 高效聚合函?rdd.groupByKey().mapValues(value => value.reduce(func))  # 常?做法rdd.reduceByKey(func)  # 更高效，因?避免了?每?主??建列表的步?// ?多? RDD 有共同的主?的?据?行分?cogroup()  # 返回RDD[(K, (Iterable[V], Iterable[W]))]// ?接rdd.join(other)rdd.leftOuterJoin(other)rdd.rightOuterJoin(other)// 自定?排序rdd.sortByKey(ascending=True, numPartitions=None, keyfunc = lambda x: str(x))// ?作rdd = sc.parallelize(({(1,2),(3,4),(3,6)}))rdd.countByKey()  # ?算主?的元素?? {(1, 1), (3, 2)}rdd.collectAsMap()  # 收集?据?map?构 Map{(1,2),(3,4),(3,6)}rdd.lookup(3)  # 返回??key的所有值 [4, 6]// ?据分?// 分布式程序中，通信十分昂?，所以??据布局?最小化网???可以大幅提高性能// 分?不是?所有的?用都有好?// 比如，??定的RDD ??描一次，那么?先分?就?有意?// ???据集需要多次重用?面向主?比如?接??的操作?才?有用// python 中不能?入 HashPartition ?象?分?，只能指定分?的?目rdd.partitionBy(100)// 自定?分?import urlparsedef hash_domain(url):return hash(urlparse.urlparse(url).netloc)rdd.partitionBy(20, hash_domain) # Create 20 partitions// 2、scala// ?建 pair RDDval pairs = lines.map(x => (x.split(" ")(0), x))val rdd = sc.parallelize(pairs)// ??值pairs.filter{case (key, value) => value.length < 20}// ?算?的均值rdd.mapValues(x => (x, 1)).reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2))// ?算?的均值（方法2）val result = input.combineByKey(    (v) => (v, 1),    (acc: (Int, Int), v) => (acc._1 + v, acc._2 + 1),    (acc1: (Int, Int), acc2: (Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2)).map{ case (key, value) => (key, value._1 / value._2.toFloat) }result.collectAsMap().map(println(_))// Word countval input = sc.textFile("s3://...")val words = input.flatMap(x => x.split(" "))val result = words.map(x => (x, 1)).reduceByKey((x, y) => x + y)// 更快的做法：?第一? RDD ?行??input.flatMap(x => x.split(" ")).countByValue()// 自定?并行度val data = Seq(("a", 3), ("b", 4), ("a", 1))sc.parallelize(data).reduceByKey((x, y) => x + y) // Default parallelismsc.parallelize(data).reduceByKey((x, y) => x + y) // Custom parallelism// 改?分?rdd.partitions.size()  // 查看分??// 自定?排序val input: RDD[(Int, Venue)] = ...implicit val sortIntegersByString = new Ordering[Int] {    override def compare(a: Int, b: Int) = a.toString.compare(b.toString)}rdd.sortByKey()// ?据分?// join ???? RDD 的主?都做哈希，通?网??送相同主?的元素到同一?机器上，然后在??机器上按主??行?接// 能?料到 userData 表??大于每五分?一?的?小的事件日志，太浪?了：即使根本?有?化，也要每次都把 userData 表跨网?的哈希和 shuffle 一遍// 示例：用?????// Initialization code; we load the user info from a Hadoop SequenceFile on HDFS.// This distributes elements of userData by the HDFS block where they are found,// and doesn't provide Spark with any way of knowing in which partition a// particular UserID is located.val sc = new SparkContext(...)val userData = sc.sequenceFile[UserID, UserInfo]("hdfs://...").persist()// Function called periodically to process a logfile of events in the past 5 minutes;// we assume that this is a SequenceFile containing (UserID, LinkInfo) pairs.def processNewLogs(logFileName: String) {    val events = sc.sequenceFile[UserID, LinkInfo](logFileName)    val joined = userData.join(events)// RDD of (UserID, (UserInfo, LinkInfo)) pairs    val offTopicVisits = joined.filter {        case (userId, (userInfo, linkInfo)) => // Expand the tuple into its components        !userInfo.topics.contains(linkInfo.topic)    }.count()    println("Number of visits to non-subscribed topics: " + offTopicVisits)}// 使用自定?分?// events RDD 是本地的，并且只在??方法里用到，所以? event RDD 指定分?不?有改善// 在?行 join 操作?，??? events RDD ?行 shuffle，并根据 userId ?送到 userData 的哈希分???的机器上，网?上??的?据大量?少// partitionBy 返回新的 RDD，RDD 一旦?建就不?被修改// 分?空值并?任??，一般??和集群的核?一?大val sc = new SparkContext(...)val userData = sc.sequenceFile[UserID, UserInfo]("hdfs://...").partitionBy(new HashPartitioner(100)) // Create 100 partitions.persist()// 确定分?// Spark 知道?部每?操作是如何影?分?的，?自???些分?操作?建的 RDD ?置 partitioner// ?于??不能保??生已知的分?，?出的 RDD ?不?有 partitioner// 聚合操作??：根据主?跨网? shuffle ?据// 影?分?操作：???，同一?主?的元素被哈希到同一台机器val pairs = sc.parallelize(List((1,1),(2,2),(3,3)))pairs.partitioner  // Noneval partitioned = pairs.partitionBy(new spark.HashPartitioner(2))partitioned.partitioner  // spark.HashPartitioner@5147788d// PageRank：?于一?文?中的每一?，依据有多少?文??接到它?作?其重要性的度量// Assume that our neighbor list was saved as a Spark objectFileval links = sc.objectFile[(String, Seq[String])]("links")    .partitionBy(new HashPartitioner(100))    .persist()// Initialize each page's rank to 1.0; since we use mapValues, the resulting RDD will have the same partitioner as linksvar ranks = links.mapValues(v => 1.0)// Run 10 iterations of PageRankfor (i <- 0 until 10) {    val contributions = links.join(ranks).flatMap {        case (pageId, (links, rank)) =>            links.map(dest => (dest, rank / links.size))    }    ranks = contributions.reduceByKey((x, y) => x + y).mapValues(v => 0.15 + 0.85*v)}// Write out the final ranksranks.saveAsTextFile("ranks")// 自定?分?class DomainNamePartitioner(numParts: Int) extends Partitioner {    override def numPartitions: Int = numParts    override def getPartition(key: Any): Int = {        val domain = new Java.net.URL(key.toString).getHost()        val code = (domain.hashCode % numPartitions)        if (code < 0) {            code + numPartitions // Make it non-negative        } else {            code        }    }    // Java equals method to let Spark compare our Partitioner objects    override def equals(other: Any): Boolean = other match {        case dnp: DomainNamePartitioner =>            dnp.numPartitions == numPartitions        case _ =>            false    }}// 3、Java// ?建 pair RDDPairFunction<String, String, String> keyData =    new PairFunction<String, String, String>() {        public Tuple2<String, String> call(String x) {        return new Tuple2(x.split(" ")[0], x);    }};JavaPairRDD<String, String> pairs = lines.mapToPair(keyData);// ??值Function<Tuple2<String, String>, Boolean> longWordFilter =    new Function<Tuple2<String, String>, Boolean>() {        public Boolean call(Tuple2<String, String> keyValue) {        return (keyValue._2().length() < 20);    }};JavaPairRDD<String, String> result = pairs.filter(longWordFilter);// Word countJavaRDD<String> input = sc.textFile("s3://...")JavaRDD<String> words = rdd.flatMap(new FlatMapFunction<String, String>() {    public Iterable<String> call(String x) { return Arrays.asList(x.split(" ")); }});JavaPairRDD<String, Integer> result = words.mapToPair(    new PairFunction<String, String, Integer>() {        public Tuple2<String, Integer> call(String x) { return new Tuple2(x, 1); }}).reduceByKey(    new Function2<Integer, Integer, Integer>() {        public Integer call(Integer a, Integer b) { return a + b; }});// 自定?排序class IntegerComparator implements Comparator<Integer> {    public int compare(Integer a, Integer b) {        return String.valueOf(a).compareTo(String.valueOf(b))    }}rdd.sortByKey(comp)```#### 第五章 加?和保存?据常?文件格式：* 文本文件：非?构化* JSON：半?构化* CSV：?构化* SequenceFile：?构化，普通的Hadoop 文件格式，用于key/value 的?据* Protocol buffers：?构化* ObjectFile：?构化```// 1、python// 加?文本文件input = sc.textFile("file:///home/holden/repos/spark/README.md")// 保存?文本文件result.saveAsTextFile(outputFile)// 加?JSON文件import jsondata = input.map(lambda x: json.loads(x))// 保存?JSON文件(data.filter(lambda x: x['lovesPandas']).map(lambda x: json.dumps(x)).saveAsTextFile(outputFile))// 加?CVS文件import csvimport StringIOdef loadRecord(line):    input = StringIO.StringIO(line)    reader = csv.DictReader(input, fieldnames=["name", "favouriteAnimal"])    return reader.next()input = sc.textFile(inputFile).map(loadRecord)// 加?完整的CSVdef loadRecords(fileNameContents):    input = StringIO.StringIO(fileNameContents[1])    reader = csv.DictReader(input, fieldnames=["name", "favoriteAnimal"])    return readerfullFileData = sc.wholeTextFiles(inputFile).flatMap(loadRecords)// ?出CSVdef writeRecords(records):    output = StringIO.StringIO()    writer = csv.DictWriter(output, fieldnames=["name", "favoriteAnimal"])    for record in records:        writer.writerow(record)    return [output.getvalue()]pandaLovers.mapPartitions(writeRecords).saveAsTextFile(outputFile)// 加?Hive?据from pyspark.sql import HiveContexthiveCtx = HiveContext(sc)  // ?境配置rows = hiveCtx.sql("SELECT name, age FROM users")firstRow = rows.first()print firstRow.name// 用 Spark SQL 加? JSONtweets = hiveCtx.jsonFile("tweets.json")tweets.registerTempTable("tweets")results = hiveCtx.sql("SELECT user.name, text FROM tweets")// 2、Scala// 加?文本文件val input = sc.textFile("file:///home/holden/repos/spark/README.md")// ?算每?文件的平均值（?取? pair RDD，主?是文件名）val input = sc.wholeTextFiles("file://home/holden/salesFiles")val result = input.mapValues{y =>    val nums = y.split(" ").map(x => x.toDouble)    nums.sum / nums.size.toDouble}// 加?JSON文件val result = input.flatMap(record => {try {    Some(mapper.readValue(record, classOf[Person]))} catch {    case e: Exception => None}})// 保存?JSON文件result.filter(p => P.lovesPandas).map(mapper.writeValueAsString(_)).saveAsTextFile(outputFile)// 加?CSV文件import Java.io.StringReaderimport au.com.bytecode.opencsv.CSVReaderval input = sc.textFile(inputFile)val result = input.map{ line =>    val reader = new CSVReader(new StringReader(line));    reader.readNext();}// 加?完整的CSVcase class Person(name: String, favoriteAnimal: String)val input = sc.wholeTextFiles(inputFile)val result = input.flatMap{ case (_, txt) =>    val reader = new CSVReader(new StringReader(txt));    reader.readAll().map(x => Person(x(0), x(1)))}// ?出CSVpandaLovers.map(person => List(person.name, person.favoriteAnimal).toArray).mapPartitions{people =>    val stringWriter = new StringWriter();    val csvWriter = new CSVWriter(stringWriter);    csvWriter.writeAll(people.toList)    Iterator(stringWriter.toString)}.saveAsTextFile(outFile)// 加?Hive?据import org.apache.spark.sql.hive.HiveContextval hiveCtx = new org.apache.spark.sql.hive.HiveContext(sc)val rows = hiveCtx.sql("SELECT name, age FROM users")val firstRow = rows.first()println(firstRow.getString(0)) // Field 0 is the name// 用 Spark SQL 加? JSONval tweets = hiveCtx.jsonFile("tweets.json")tweets.registerTempTable("tweets")val results = hiveCtx.sql("SELECT user.name, text FROM tweets")// 使用JdbcRDDdef createConnection() = {    Class.forName("com.mysql.jdbc.Driver").newInstance();    DriverManager.getConnection("jdbc:mysql://localhost/test?user=holden");}def extractValues(r: ResultSet) = {    (r.getInt(1), r.getString(2))}val data = new JdbcRDD(sc,    createConnection, "SELECT * FROM panda WHERE ? <= id AND id <= ?",    lowerBound = 1, upperBound = 3, numPartitions = 2, mapRow = extractValues)    println(data.collect().toList)// 3、JavaJavaRDD<String> input = sc.textFile("file:///home/holden/repos/spark/README.md")```#### 第五章 Spark高??程共享?量：* 累加器（在??中使用，而不是?作）* ?播（?可能?少?播?容，?量??送到每????一次，并且是只?的）```// 1、python// 累加器?算空行?file = sc.textFile(inputFile)blankLines = sc.accumulator(0)  # Create Accumulator[Int] initialized to 0def extractCallSigns(line):    global blankLines  # Make the global variable accessible    if (line == ""):        blankLines += 1    return line.split(" ")callSigns = file.flatMap(extractCallSigns)callSigns.saveAsTextFile(outputDir + "/callsigns")  # 真正?行print "Blank lines: %d" % blankLines.value  # ??值// 用?播?量查找?家signPrefixes = sc.broadcast(loadCallSignTable())  # ?播?量def processSignCount(sign_count, signPrefixes):    country = lookupCountry(sign_count[0], signPrefixes.value)  // ??值    count = sign_count[1]    return (country, count)countryContactCounts = (contactCounts    .map(processSignCount)    .reduceByKey((lambda x, y: x+ y)))countryContactCounts.saveAsTextFile(outputDir + "/countries.txt")// 2、scala// 累加器?算空行?val sc = new SparkContext(...)val file = sc.textFile("file.txt")val blankLines = sc.accumulator(0)  // Create an Accumulator[Int] initialized to 0val callSigns = file.flatMap(line => {    if (line == "") {        blankLines += 1 // Add to the accumulator    }    line.split(" ")})callSigns.saveAsTextFile("output.txt")  // 真正?行println("Blank lines: " + blankLines.value)// 用?播?量查找?家val signPrefixes = sc.broadcast(loadCallSignTable())  // ?播?量val countryContactCounts = contactCounts.map{case (sign, count) =>    val country = lookupInArray(sign, signPrefixes.value)    (country, count)}.reduceByKey((x, y) => x + y)countryContactCounts.saveAsTextFile(outputDir + "/countries.txt")```基于分?工作：* 避免?每??据做重复?置* 分?版本的 map 和 foreach 函?可? RDD 的每?分???行?些代?一次```// 1、python// 共享?接池def processCallSigns(signs):    http = urllib3.PoolManager()    urls = map(lambda x: "http://73s.com/qsos/%s.json" % x, signs)    requests = map(lambda x: (x, http.request('GET', x)), urls)    result = map(lambda x: (x[0], json.loads(x[1].data)), requests)    return filter(lambda x: x[1] is not None, result)def fetchCallSigns(input):    return input.mapPartitions(lambda callSigns : processCallSigns(callSigns))contactsContactList = fetchCallSigns(validSigns)// ?算均值（每?分??建元?一次）def combineCtrs(c1, c2):    return (c1[0] + c2[0], c1[1] + c2[1])def basicAvg(nums):    nums.map(lambda num: (num, 1)).reduce(combineCtrs)// 2、scala// 共享?接池val contactsContactLists = validSigns.distinct().mapPartitions{signs =>    val mapper = createMapper()    val client = new HttpClient()    client.start()    signs.map {sign =>    createExchangeForSign(sign)    }.map{ case (sign, exchange) =>        (sign, readExchangeCallLog(mapper, exchange))    }.filter(x => x._2 != null) // Remove empty CallLogs}// ?算均值（每?分??建元?一次）def partitionCtr(nums):    sumCount = [0, 0]    for num in nums:        sumCount[0] += num        sumCount[1] += 1    return [sumCount]def fastAvg(nums):    sumCount = nums.mapPartitions(partitionCtr).reduce(combineCtrs)    return sumCount[0] / float(sumCount[1])```管道?出到外部系?：* Python、Java 或 Scala* ?送?据到其他?言?的程序，如 R ?言?本?值 RDD 操作：* count()：??* mean()：平均值* sum()：和* max()：最大值* min()：最小值* variance()：方差* sampleVariance()：?本方差* stdev()：?准?差* sampleStdev()：?本?准?差```// 1、python// 使用????去除?据中的异常值distanceNumerics = distances.map(lambda string: float(string))stats = distanceNumerics.stats()stddev = std.stdev()mean = stats.mean()reasonableDistances = distanceNumerics.filter(lambda x: math.fabs(x - mean) < 3 * stddev)  # 移除异常值print reasonableDistances.collect()  # ?存// 2、scala// 使用????去除?据中的异常值val distanceDouble = distance.map(string => string.toDouble)val stats = distanceDoubles.stats()val stddev = stats.stdevval mean = stats.meanval reasonableDistances = distanceDoubles.filter(x => math.abs(x-mean) < 3 * stddev)  // 移除异常值println(reasonableDistance.collect().toList)  // ?存```#### 第七章 在集群上?行使用 Spark 的高?? API 的好?：* ?箱即用，在本地小?据集上快速???用原型，然后不用修改代?就能在非常大的集群上?行Spark ?行架构：* 分布式模式中，Spark 使用主?架构* 有一?中心??器 driver 和?多分布式 executor* master 和 worker 用于描述集群管理器的中心和分布的部分driver：* main 方法所在的?程，一旦 driver ?止，整??用就?束了* ??用?程序到任?* ?度 task 到 executorexecutor：* ?行?定的 Spark 作?中的??任?，并返回?果到 driver* ?用?程序中?存的 RDD 提供?存存?* 任?可以和?据在一起?行集群管理器：* Spark 依?于集群管理器??? executor，并且有?候也?? driver* 是可拔插的?件。?允? Spark ?行在不同的外部管理器上面??一?程序：* 不?使用哪?集群管理器，Spark 提供了 spark-submit ?本用于提交程序* 通?各种??，?接到不同的集群管理器，控制?用?得多少?源Spark ?用在集群?行?的准确步?：* 用?用spark-submit 提交了一??用* spark-submit ?? driver 程序，并?用用?指定的 main() 方法* driver 程序?系集群管理器?求?源???各 executor* 集群管理器代表 driver 程序??各 executor* Driver ?程?行整?用??用。程序中基于 RDD 的??和?作，driver 程序以task 的形式?送到各 executor* Task 在 executor ?程?行??算和保存?果* 如果 driver 的 main() 方法退出或者?用了 SparkContext.stop()，就??止 executor 的?行并?放?集群管理器分配的?源用 spark-submit 分??用```// 提交 python ?用bin/spark-submit my_script.py// 指定集群提交?用bin/spark-submit --master spark://host:7077 --executor-memory 10g my_script.py-- master spark://host:port-- master mesos://host:port-- master yarn-- master local-- master local[N]-- master local[*]// spark-submit 的一般格式bin/spark-submit [options] <app jar | python file> [app options]--master--deploy-mode--class--name--jars--files--py-files--executor-memory--driver-memory$ ./bin/spark-submit \--master spark://hostname:7077 \--deploy-mode cluster \--class com.databricks.examples.SparkExample \--name "Example Program" \--jars dep1.jar,dep2.jar,dep3.jar \--total-executor-cores 300 \--executor-memory 10g \myApp.jar "options" "to your application" "go here"# Submitting a Python application in YARN client mode$ export HADOP_CONF_DIR=/opt/hadoop/conf$ ./bin/spark-submit \--master yarn \--py-files somelib-1.2.egg,otherlib-4.4.zip,other-file.py \--deploy-mode client \--name "Example Program" \--queue exampleQueue \--num-executors 40 \--executor-memory 10g \my_script.py "options" "to your application" "go here"// Maven 构建 Spark ?目$ mvn package$ ls target/example-build-1.0.jaroriginal-example-build-1.0.jar$ jar tf target/example-build-1.0.jar...joptsimple/HelpFormatter.class...org/joda/time/tz/UTCProvider.class...$ /path/to/spark/bin/spark-submit --master local ... target/example-build-1.0.jar// sbt 构建 Scala 的 Spark ?用import AssemblyKeys._name := "Simple Project"version := "1.0"organization := "com.databricks"scalaVersion := "2.10.3"libraryDependencies ++= Seq(    "org.apache.spark" % "spark-core_2.10" % "1.2.0" % "provided",    "net.sf.jopt-simple" % "jopt-simple" % "4.3",    "joda-time" % "joda-time" % "2.0")assemblySettingsjarName in assembly := "my-project-assembly.jar"assemblyOption in assembly :=    (assemblyOption in assembly).value.copy(includeScala = false)```依??突：* 修改?用，使用和 Spark 使用的相同版本的第三方?* 用 shading ?修改?用的包多用?集群的?度：* 集群管理器，共享?用之?的?源集群管理器：* ?机：Standalong 模式* 集群：Hadoop YARN 和 Apache Mesos#### 第八章 Spark的?优和??配置信息：（优先??大到小）* 通? set 方法?置* ?行????置Spark Web：* job ?面：stage，task ?斜* Storage：持久化的 RDD 信息* Exexutors：?用在存在的 executor 列表* Environment：?? Spark 的配置* Driver 和 Executor 的日志??性能考量：* 并行度* 序列化格式：文件??格式* ?存管理：?存用于 RDD 存?、Shuffle 和聚合?存、用?代?* 硬件配置：?每? executor 的?存?量，每? executor 的核?，? executor ??，以及用于???据的本地磁??```// 1、Pythonconf = new SparkConf()conf.set("spark.app.name", "My SparkApp")conf.set("spark.master", "local[4]")conf.set("spark.ui.port", "36000")conf.setMaster()sc = SparkContext(conf)// 2、Scalaval conf = new SparkConf()conf.set("spark.app.name", "My SparkApp")conf.set("spark.master", "local[4]")conf.set("spark.ui.port", "36000")conf.setMaster()val sc = new SparkContext(conf)// ?行???$ bin/spark-submit \--class com.example.MyApp \--master local[4] \--name "My SparkApp" \--conf spark.ui.port=36000 \myApp.jar```